train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
train[complete.cases(train)==FALSE,]
param <- list("objective" = "multi:softmax",    # multiclass classification
"num_class" = 5,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.2,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85,  # subsample ratio of columns when constructing each tree
"num_class" = 3
)
t.xgb.cv = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
min.merror.idx
t.xgb.cv$test.merror.std
which.min(t.xgb.cv$test.merror.std)
t.xgb <- xgboost(data = train, label = FTR, params = param, nround = which.min(t.xgb.cv$test.merror.std),verbose = 0,num_class = 3)
t.xgb.pr = predict(t.xgb,test)
library(xgboost)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
View(train)
str(train$FTR)
levels = levels(train$FTR)
FTR = as.numeric(train$FTR)-1
ID = test$ID
train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
param <- list("objective" = "multi:softmax",    # multiclass classification
"num_class" = 5,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.2,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85,  # subsample ratio of columns when constructing each tree
"num_class" = 3
)
xgb.cv.1 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.1$test.merror.mean)
set.seed(123)
xgb.cv.1 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.1$test.merror.mean)
t.xgb.1 <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.1$test.merror.std),verbose = 0,num_class = 3)
t.xgb.pr.1 = predict(t.xgb.1,test)
?ifelse
ifelse(t.xgb.pr.1==0,"A")
ifelse(t.xgb.pr.1==0,"A",ifelse(t.xgb.pr.1==1),"D","H")
ifelse(t.xgb.pr.1==0,"A",ifelse(t.xgb.pr.1==1,"D","H"))
t.xgb.pr.1
t.xgb.pr.1 = ifelse(t.xgb.pr.1==0,"A",ifelse(t.xgb.pr.1==1,"D","H"))
result.1 = dataframe(ID,t.xgb.pr.1)
result.1 = data.frame(ID,t.xgb.pr.1)
View(result.1)
colname(result.1) = c("ID","FTR")
colnames(result.1) = c("ID","FTR")
write.csv(result.1,"1.csv",col.names = T)
write.csv(result.1,"1.csv",row.names = F)
table(result.1$FTR)
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
ID = test$ID
train$ID = NULL
test$ID = NULL
View(xgb.cv.1)
t.rf.2 = randomForest(FTR~.,data = train,importance = TRUE) #missing values need to be omitted, otherwise warnings will present
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.factor(month(dt$Date))
dt$Wday = as.factor(wday(dt$Date))
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
ID = test$ID
train$ID = NULL
test$ID = NULL
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = month(dt$Date)
dt$Wday = wday(dt$Date)
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
ID = test$ID
train$ID = NULL
test$ID = NULL
table(train$Month)
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Month = as.factor(month(dt$Date))
dt$Date = as.POSIXct(dt$Date)
dt$Wday = as.factor(wday(dt$Date))
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
ID = test$ID
train$ID = NULL
test$ID = NULL
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.factor(as.numeric(month(dt$Date)))
dt$Wday = as.factor(as.numeric(wday(dt$Date)))
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
table(train$Month)
t.rf.2 = randomForest(FTR~.,data = train,importance = TRUE) #missing values need to be omitted, otherwise warnings will present
t.rf.pr.2= predict(t.rf.2,test)
result.2=data.frame(ID,rf.predict)
t.rf.pr.2= predict(t.rf.2,test)
result.2=data.frame(ID,t.rf.pr.2)
colnames(result.2)=c("ID","FTR")
View(result.1)
table(result.1$FTR)
table(result.2$FTR)
write.table(result.2,file="2.csv",col.names = TRUE, row.names = FALSE, sep = ",")
library(xgboost)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
FTR = as.numeric(train$FTR)-1
ID = test$ID
train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
param <- list("objective" = "multi:softmax",    # multiclass classification
param <- list("objective" = "multi:softmax",    # multiclass classification
param <- list("objective" = "multi:softmax",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.2,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(8)
set.seed(8)
xgb.cv.3 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
param <- list("objective" = "multi:softmax",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.1,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(8)
xgb.cv.3 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.3$test.merror.mean)
?combine
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "mlogloss",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.1,    # step size shrinkage
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
"subsample" = 0.85,    # part of data instances to grow tree
)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "mlogloss",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.1,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(8)
xgb.cv.3 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
t.xgb.3 <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.3$test.merror.std),verbose = 0,num_class = 3)
which.min(xgb.cv.3$test.merror.mean)
which.min(xgb.cv.3$test.mlogloss.mean)
preds.3 <- vector("list", length = 100)
for(i in 1:100){
print(paste('training model:', i))
model <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.3$test.mlogloss.mean),verbose = 0,num_class = 3)
print(paste('applying prediction:', i))
preds[[i]] <- predict(model, newdata = test)
}
for(i in 1:100){
print(paste('training model:', i))
model <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.3$test.mlogloss.mean),verbose = 0,num_class = 3)
print(paste('applying prediction:', i))
preds.3[[i]] <- predict(model, newdata = test)
}
com_preds.100 <- colMeans(do.call(rbind, preds.3))
result.4 <- matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T)
View(result.4)
library(plyr)
?rowwise
rowise
which.max(result.4[,1])
which.max(result.4[1,])
which.max(result.4[2,])
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T),"FTR" = character(610)
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T),"FTR" = character(610))
View(result.4)
for(i in (1:640)){
ifelse(which.max(result.4[i,]==1),result.4$FTR[i]=="A",ifelse(which.max(result.4[i,]==2),result.4$FTR[i]=="D",result.4$FTR[i]=="H"))
}
for(i in (1:640)){
ifelse(which.max(result.4[i,]==1),result.4$FTR[i]="A",ifelse(which.max(result.4[i,]==2),result.4$FTR[i]="D",result.4$FTR[i]="H"))
}
```
for(i in (1:640)){
ifelse(which.max(result.4[i,]==1),result.4$FTR[i]="A",ifelse(which.max(result.4[i,]==2),result.4$FTR[i]="D",result.4$FTR[i]="H"))
}
for(i in (1:640)){
ifelse(which.max(result.4[i,]==1),result.4$FTR[i]="A",ifelse(which.max(result.4[i,]==2),result.4$FTR[i]="D",result.4$FTR[i]="H"))
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T),"FTR" = character(610))
for(i in (1:640)){
ifelse(which.max(result.4[i,])==1,result.4$FTR[i]="A",ifelse(which.max(result.4[i,])==2,result.4$FTR[i]="D",result.4$FTR[i]="H"))
i=1
ifelse(which.max(result.4[i,])==1,result.4$FTR[i]="A",ifelse(which.max(result.4[i,])==2,result.4$FTR[i]="D",result.4$FTR[i]="H"))
which.max(result.4[i,])==1
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T),"FTR" = 0)
which.max(result.4[i,])==1
which.max(result.4[1,])
(which.max(result.4[i,])=="X1"
which.max(result.4[i,])=="X1"
which.max(result.4[i,])=="X3"
str(which.max(result.4[1,]))
View(result.4)
which.max(result.4[2,])
library(xgboost)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
FTR = as.numeric(train$FTR)-1
ID = test$ID
train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "mlogloss",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.1,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(8)
xgb.cv.3 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.3$test.mlogloss.mean)
preds.3 <- vector("list", length = 100)
for(i in 1:100){
print(paste('training model:', i))
model <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.3$test.mlogloss.mean),verbose = 0,num_class = 3)
print(paste('applying prediction:', i))
preds.3[[i]] <- predict(model, newdata = test)
}
com_preds.100 <- colMeans(do.call(rbind, preds.3))
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T),"FTR" = 0)
which.max(result.4[1,])
attributes(which.max(result.4[1,]))
which.max(result.4[1,])==3
for(i in (1:640)){
ifelse(which.max(result.4[i,])==1,result.4$FTR[i]="A",ifelse(which.max(result.4[i,])==2,result.4$FTR[i]="D",result.4$FTR[i]="H"))
?ifelse
result.4 <- data.frame(matrix(com_preds.100,nrow = 610, ncol = 3, byrow = T)
result.4 = c(1:610)
result.4[1]
result.4=c()
result.4=c()
for(i in (1:640)){
result.4 = ifelse(which.max(com_preds.100[i,])==1,"A",ifelse(which.max(com_preds.100[i,])==2,"D","H"))
}
com_preds.100 <- matrix(colMeans(do.call(rbind, preds.3)),nrow = 610, ncol = 3, byrow = T)
View(com_preds.100)
result.3=c()
for(i in (1:640)){
result.3 = ifelse(which.max(com_preds.100[i,])==1,"A",ifelse(which.max(com_preds.100[i,])==2,"D","H"))
}
result.3=c()
for(i in (1:610)){
result.3 = ifelse(which.max(com_preds.100[i,])==1,"A",ifelse(which.max(com_preds.100[i,])==2,"D","H"))
}
result.3=c()
for(i in (1:610)){
result.3[i] = ifelse(which.max(com_preds.100[i,])==1,"A",ifelse(which.max(com_preds.100[i,])==2,"D","H"))
}
library(xgboost)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
FTR = as.numeric(train$FTR)-1
ID = test$ID
train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
param <- list("objective" = "multi:softmax",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.2,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(123)
xgb.cv.1 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.1$test.merror.mean)
t.xgb.1 <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.1$test.merror.std),verbose = 0,num_class = 3)
t.xgb.pr.1 = predict(t.xgb.1,test)
xgb.cv.1 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.1$test.merror.mean)
t.xgb.1 <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.1$test.merror.std),verbose = 0,num_class = 3)
t.xgb.pr.1 = predict(t.xgb.1,test)
t.xgb.pr.1 = ifelse(t.xgb.pr.1==0,"A",ifelse(t.xgb.pr.1==1,"D","H"))
result.1 = data.frame(ID,t.xgb.pr.1)
colnames(result.1) = c("ID","FTR")
library(randomForest)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.factor(as.numeric(month(dt$Date)))
dt$Wday = as.factor(as.numeric(wday(dt$Date)))
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
ID = test$ID
train$ID = NULL
test$ID = NULL
t.rf.2 = randomForest(FTR~.,data = train,importance = TRUE) #missing values need to be omitted, otherwise warnings will present
t.rf.pr.2= predict(t.rf.2,test)
result.2=data.frame(ID,t.rf.pr.2)
colnames(result.2)=c("ID","FTR")
library(xgboost)
train = read.csv("train.csv",header = TRUE,sep=",")
test = read.csv("test.csv",header = TRUE,sep=",")
library(lubridate)
convert = function(dt){
dt$Date = as.POSIXct(dt$Date)
dt$Month = as.numeric(as.factor(month(dt$Date)))-1
dt$Wday = as.numeric(as.factor(wday(dt$Date)))-1
dt$Date = NULL
dt$HomeTeam = NULL
dt$AwayTeam = NULL
dt[is.na(dt)]=0 #replace all NA with 0
return(dt)
}
train = convert(train)
test = convert(test)
levels = levels(train$FTR)
FTR = as.numeric(train$FTR)-1
ID = test$ID
train$ID = NULL
test$ID = NULL
train$FTR=NULL
train=as.matrix(train)
test=as.matrix(test)
##Model Training
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 3,    # number of classes
"eval_metric" = "mlogloss",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"eta" = 0.1,    # step size shrinkage
"subsample" = 0.85,    # part of data instances to grow tree
"colsample_bytree" = 0.85  # subsample ratio of columns when constructing each tree
)
set.seed(8)
xgb.cv.3 = xgb.cv(params = param,data = train,label = FTR,nfold = 5,nrounds = 200)
which.min(xgb.cv.3$test.mlogloss.mean)
preds.3 <- vector("list", length = 100)
for(i in 1:100){
print(paste('training model:', i))
model <- xgboost(data = train, label = FTR, params = param, nround = which.min(xgb.cv.3$test.mlogloss.mean),verbose = 0,num_class = 3)
print(paste('applying prediction:', i))
preds.3[[i]] <- predict(model, newdata = test)
}
com_preds.100 <- matrix(colMeans(do.call(rbind, preds.3)),nrow = 610, ncol = 3, byrow = T)
result.3=c()
for(i in (1:610)){
result.3[i] = ifelse(which.max(com_preds.100[i,])==1,"A",ifelse(which.max(com_preds.100[i,])==2,"D","H"))
}
result.3=data.frame(ID,result.3)
colnames(result.3)=c("ID","FTR")
table(result.1$FTR)
table(result.2$FTR)
table(result.3$FTR)
write.table(result.3,file="3.csv",col.names = TRUE, row.names = FALSE, sep = ",")
